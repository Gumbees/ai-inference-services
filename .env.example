# ============================================
# AI Inference Services - Environment Configuration
# ============================================
# Copy this file to .env and configure your values

# --------------------------------------------
# Traefik Domain Configuration
# --------------------------------------------

# Whisper (Speech-to-Text)
WHISPER_PUBLIC_DOMAIN=whisper.example.com
WHISPER_PRIVATE_DOMAIN=whisper.local.example.com

# Ollama (LLM Inference)
OLLAMA_PUBLIC_DOMAIN=ollama.example.com
OLLAMA_PRIVATE_DOMAIN=ollama.local.example.com

# Hindsight (AI Memory Layer)
HINDSIGHT_API_DOMAIN=hindsight-api.example.com
HINDSIGHT_API_INTERNAL_DOMAIN=hindsight-api.local.example.com
HINDSIGHT_UI_DOMAIN=hindsight.example.com
HINDSIGHT_UI_INTERNAL_DOMAIN=hindsight.local.example.com

# --------------------------------------------
# Hindsight Configuration
# --------------------------------------------
# Vectorize Hindsight - AI memory with MCP support
# GitHub: https://github.com/vectorize-io/hindsight
# MCP endpoint: https://{HINDSIGHT_API_DOMAIN}/mcp/{user_id}/

# LLM Model (must support 65k+ output tokens)
# Default uses Ollama with qwen2.5:7b
HINDSIGHT_LLM_MODEL=qwen2.5:7b

# Log level (debug, info, warning, error)
HINDSIGHT_LOG_LEVEL=info
